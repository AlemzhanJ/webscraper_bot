import asyncio
import logging
import aiohttp # Use aiohttp for async requests
# Removed ssl import and patch
from bs4 import BeautifulSoup
import io
import re
from urllib.parse import urlparse, urljoin
from collections import deque # Use deque for queue
import datetime

from aiogram import Bot, Dispatcher, types
from aiogram.client.default import DefaultBotProperties
from aiogram.client.session.aiohttp import AiohttpSession # Import Aiogram's session wrapper
from aiogram.filters import CommandStart, Command
from aiogram.types import Message, BufferedInputFile, InlineKeyboardMarkup, InlineKeyboardButton, ReplyKeyboardMarkup, KeyboardButton
from aiogram.enums import ParseMode
from aiogram.fsm.context import FSMContext
from aiogram.fsm.storage.memory import MemoryStorage
from aiogram.fsm.state import State, StatesGroup

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–∞—à–∏ –º–æ–¥—É–ª–∏
from database import init_db, get_db, User, AISession, Message as DBMessage, get_cached_document, cache_document, get_cached_summary, cache_summary, cleanup_old_cache, CachedDocument, CachedSummary, cache_stats
from sqlalchemy import func
from ai_client import AIClient
from config import BOT_TOKEN, GEMINI_API_KEY, AI_MODEL, GEMINI_API_URL, AI_SYSTEM_PROMPT_TEMPLATE, MAX_SESSION_REQUESTS, SESSION_TIMEOUT_MINUTES, AI_SUMMARY_PROMPT, MAX_MESSAGE_LENGTH, ADMIN_IDS
from rate_limiter import rate_limiter

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏—è –¥–ª—è FSM
class BotStates(StatesGroup):
    NORMAL = State()  # –û–±—ã—á–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
    AI_CHAT = State()  # –†–µ–∂–∏–º —á–∞—Ç–∞ —Å –ò–ò

# Initialize dispatcher globally with FSM storage
storage = MemoryStorage()
dp = Dispatcher(storage=storage)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞ –ò–ò
ai_client = AIClient(api_key=GEMINI_API_KEY, base_url=GEMINI_API_URL, model=AI_MODEL)

# Basic URL validation regex - Escaped hyphens inside character sets
URL_REGEX = r'^(https?://)?([\w\-]+\.)+[\w\-]+(/[\w\- ./?%&=]*)?$'

def is_valid_url(url):
    """Checks if the provided string is a valid URL."""
    # Basic check, can be improved
    return re.match(URL_REGEX, url) is not None

def get_text_from_html(html_content: str) -> str:
    """Extracts text content from HTML string with preserved structure."""
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # Remove script, style, and nav elements
    for element in soup(["script", "style", "nav", "footer", "aside"]):
        element.decompose()
    
    # –î–æ–±–∞–≤–∏–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
    for header in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å –∑–∞–≥–æ–ª–æ–≤–∫–∞ –ø–æ —Ç–µ–≥—É
        level = int(header.name[1])
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —É—Ä–æ–≤–Ω—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
        if level == 1:
            header.insert_before(soup.new_string('\n\n‚òÖ' + '‚ïê' * 48 + '‚òÖ\n'))
            header.insert_after(soup.new_string('\n‚òÖ' + '‚ïê' * 48 + '‚òÖ\n'))
        elif level == 2:
            header.insert_before(soup.new_string('\n\n‚îå' + '‚îÄ' * 38 + '‚îê\n'))
            header.insert_after(soup.new_string('\n‚îî' + '‚îÄ' * 38 + '‚îò\n'))
        else:
            header.insert_before(soup.new_string('\n\n‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢\n'))
            header.insert_after(soup.new_string('\n‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢\n'))
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ –¥–ª—è –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
    for paragraph in soup.find_all('p'):
        paragraph.insert_after(soup.new_string('\n\n'))
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ –¥–ª—è —Å–ø–∏—Å–∫–æ–≤
    for ul in soup.find_all('ul'):
        ul.insert_before(soup.new_string('\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n'))
        ul.insert_after(soup.new_string('\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n'))
    
    for ol in soup.find_all('ol'):
        ol.insert_before(soup.new_string('\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n'))
        ol.insert_after(soup.new_string('\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n'))
    
    for li in soup.find_all('li'):
        if li.parent.name == 'ol':
            # –î–ª—è –Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–ø–∏—Å–∫–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ü–∏—Ñ—Ä—ã
            li.insert_before(soup.new_string('\n  ‚ûú '))
        else:
            # –î–ª—è –Ω–µ–Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–ø–∏—Å–∫–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥—Ä—É–≥–æ–π –º–∞—Ä–∫–µ—Ä
            li.insert_before(soup.new_string('\n  ‚óÜ '))
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ –¥–ª—è —Ç–∞–±–ª–∏—Ü
    for table in soup.find_all('table'):
        table.insert_before(soup.new_string('\n\n‚îè' + '‚îÅ' * 30 + ' –¢–ê–ë–õ–ò–¶–ê ' + '‚îÅ' * 30 + '‚îì\n'))
        table.insert_after(soup.new_string('\n‚îó' + '‚îÅ' * 70 + '‚îõ\n\n'))
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Å—ã–ª–∫–∞—Ö
    for a in soup.find_all('a', href=True):
        a.insert_after(soup.new_string(f" üîó [{a['href']}]"))
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    for img in soup.find_all('img'):
        alt_text = img.get('alt', '–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ')
        img.insert_before(soup.new_string(f'\n[üñºÔ∏è –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ï: {alt_text}]\n'))
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–ª–æ–∫–∏ –∫–æ–¥–∞
    for code in soup.find_all('code'):
        code.insert_before(soup.new_string('\n```\n'))
        code.insert_after(soup.new_string('\n```\n'))
    
    for pre in soup.find_all('pre'):
        pre.insert_before(soup.new_string('\n```\n'))
        pre.insert_after(soup.new_string('\n```\n'))
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ü–∏—Ç–∞—Ç—ã
    for blockquote in soup.find_all('blockquote'):
        blockquote.insert_before(soup.new_string('\n\n‚ñå '))
        lines = blockquote.get_text().split('\n')
        # –ó–∞–º–µ–Ω—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ blockquote –Ω–∞ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ
        blockquote.clear()
        for line in lines:
            blockquote.append(f'\n‚ñå {line}')
        blockquote.insert_after(soup.new_string('\n\n'))
    
    # Get text with preserved structure
    text = soup.get_text()
    
    # Break into lines and remove leading/trailing space on each
    lines = (line.strip() for line in text.splitlines())
    
    # Remove duplicated empty lines but keep meaningful structure
    processed_lines = []
    prev_line_empty = False
    
    for line in lines:
        is_empty = len(line) == 0
        is_separator = any(c in '‚îÄ‚ïê‚îÅ‚Ä¢‚îå‚îê‚îî‚îò‚îè‚îì‚îó‚îõ‚òÖ‚óÜ‚ûú' for c in line)
        
        # –í—Å–µ–≥–¥–∞ –¥–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏
        if is_separator:
            processed_lines.append(line)
            prev_line_empty = False
        # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –ø—Ä–µ–¥—ã–¥—É—â–∞—è –Ω–µ –±—ã–ª–∞ –ø—É—Å—Ç–æ–π
        elif is_empty:
            if not prev_line_empty:
                processed_lines.append(line)
                prev_line_empty = True
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
        else:
            processed_lines.append(line)
            prev_line_empty = False
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –æ–¥–∏–Ω–∞—Ä–Ω—ã–º–∏ –ø–µ—Ä–µ–Ω–æ—Å–∞–º–∏ —Å—Ç—Ä–æ–∫
    text = '\n'.join(processed_lines)
    
    return text

def clean_text_for_telegram(text):
    """
    –û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç —Å–∏–º–≤–æ–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –≤—ã–∑–≤–∞—Ç—å –ø—Ä–æ–±–ª–µ–º—ã –≤ Telegram.
    –£–¥–∞–ª—è–µ—Ç * –∏ # –∏ –¥—Ä—É–≥–∏–µ –º–∞—Ä–∫–¥–∞—É–Ω —Å–∏–º–≤–æ–ª—ã.
    """
    # –£–¥–∞–ª—è–µ–º –º–∞—Ä–∫–¥–∞—É–Ω —Å–∏–º–≤–æ–ª—ã
    text = re.sub(r'[*#]', '', text)
    
    # –£–¥–∞–ª–µ–Ω–∏–µ HTML —Ç–µ–≥–æ–≤
    text = re.sub(r'<[^>]+>', '', text)
    
    # –£–¥–∞–ª–µ–Ω–∏–µ –Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —Ç–µ–≥–æ–≤ Telegram
    text = re.sub(r'<(provider|script|style).*?>.*?</\1>', '', text, flags=re.DOTALL)
    
    # –ó–∞–º–µ–Ω–∞ –æ–±—Ä–∞—Ç–Ω—ã—Ö —Å–ª–µ—à–µ–π –∏ –¥—Ä—É–≥–∏—Ö —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
    text = text.replace('\\', '\\\\').replace('`', '\\`')
    
    return text

def split_long_message(text, max_length=MAX_MESSAGE_LENGTH):
    """
    –†–∞–∑–¥–µ–ª—è–µ—Ç –¥–ª–∏–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –Ω–∞ —á–∞—Å—Ç–∏, —á—Ç–æ–±—ã —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º Telegram.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —á–∞—Å—Ç–µ–π —Å–æ–æ–±—â–µ–Ω–∏—è.
    """
    if len(text) <= max_length:
        return [text]
    
    parts = []
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∞–±–∑–∞—Ü–∞–º, —á—Ç–æ–±—ã –Ω–µ —Ä–∞–∑—Ä—ã–≤–∞—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    paragraphs = text.split('\n\n')
    current_part = ""
    
    for paragraph in paragraphs:
        # –ï—Å–ª–∏ –∞–±–∑–∞—Ü —Å–∞–º –ø–æ —Å–µ–±–µ –±–æ–ª—å—à–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã, —Ä–∞–∑–±–∏–≤–∞–µ–º –µ–≥–æ –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º
        if len(paragraph) > max_length:
            sentences = re.split(r'([.!?])\s+', paragraph)
            i = 0
            while i < len(sentences):
                if i + 1 < len(sentences):
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å –µ–≥–æ –∑–Ω–∞–∫–æ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
                    sentence = sentences[i] + sentences[i + 1]
                    i += 2
                else:
                    sentence = sentences[i]
                    i += 1
                
                if len(current_part + "\n\n" + sentence) > max_length:
                    parts.append(current_part)
                    current_part = sentence
                else:
                    if current_part:
                        current_part += "\n\n"
                    current_part += sentence
        else:
            # –î–æ–±–∞–≤–ª—è–µ–º –∞–±–∑–∞—Ü —Ü–µ–ª–∏–∫–æ–º, –µ—Å–ª–∏ –æ–Ω –ø–æ–º–µ—â–∞–µ—Ç—Å—è
            if len(current_part + "\n\n" + paragraph) > max_length:
                parts.append(current_part)
                current_part = paragraph
            else:
                if current_part:
                    current_part += "\n\n"
                current_part += paragraph
    
    # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —á–∞—Å—Ç—å
    if current_part:
        parts.append(current_part)
    
    return parts

async def fetch_page(session: aiohttp.ClientSession, url: str) -> tuple[str | None, str | None]:
    """Fetches a single page asynchronously."""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'
    }
    try:
        async with session.get(url, headers=headers, timeout=10, ssl=False) as response: # Added ssl=False for potential issues
            response.raise_for_status()
            content_type = response.headers.get('content-type', '').lower()
            if 'text/html' in content_type:
                return await response.text(), None
            else:
                return None, f"Content type is not HTML ({content_type})"
    except aiohttp.ClientError as e:
        logger.warning(f"HTTP Error fetching {url}: {e}")
        return None, f"HTTP Error: {e}"
    except asyncio.TimeoutError:
        logger.warning(f"Timeout fetching {url}")
        return None, "Timeout"
    except Exception as e:
        logger.error(f"Unexpected error fetching {url}: {e}")
        return None, f"Unexpected error: {e}"


async def crawl_website(start_url: str, max_pages: int = 1000, status_message: Message = None) -> tuple[str, int]:
    """Crawls a website starting from start_url, collecting text."""
    logger.info(f"Starting crawl for: {start_url} (max_pages={max_pages})")
    if not start_url.startswith(('http://', 'https://')):
        start_url = 'https://' + start_url

    parsed_start_url = urlparse(start_url)
    base_domain = parsed_start_url.netloc
    if not base_domain:
        return "Error: Invalid starting URL.", 0

    to_visit = deque([start_url])
    visited = set()
    scraped_data = {}
    pages_processed = 0
    errors = []
    
    # –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –¥–∏–Ω–∞–º–∏–∫–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –Ω–æ–≤—ã—Ö URL
    total_links_found = 0
    new_links_by_page = {}
    consecutive_low_discovery_pages = 0
    discovery_threshold = 3  # –ü–æ—Ä–æ–≥ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü —Å –Ω–∏–∑–∫–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –Ω–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫
    max_consecutive_low_pages = 5  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–¥—Ä—è–¥ –∏–¥—É—â–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü —Å –Ω–∏–∑–∫–∏–º –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ–º
    
    # –î–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
    last_update_time = datetime.datetime.now()
    update_interval = datetime.timedelta(seconds=2)  # –û–±–Ω–æ–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –∫–∞–∂–¥—ã–µ 2 —Å–µ–∫—É–Ω–¥—ã
    
    # –û—Ü–µ–Ω–∫–∞ –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–∞–Ω–∏—Ü (–Ω–∞—á–∏–Ω–∞–µ–º —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –≤ –æ—á–µ—Ä–µ–¥–∏)
    estimated_total_pages = len(to_visit)

    async with aiohttp.ClientSession() as session:
        while to_visit and pages_processed < max_pages:
            current_url = to_visit.popleft()

            if current_url in visited:
                continue

            # Normalize URL slightly (e.g., remove fragment)
            current_url = urljoin(current_url, urlparse(current_url).path)
            if not current_url.startswith(('http://', 'https://')): # Ensure scheme is present
                 continue

            if current_url in visited:
                continue

            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            current_time = datetime.datetime.now()
            if status_message and (current_time - last_update_time > update_interval or pages_processed == 0):
                last_update_time = current_time
                
                # –ï—Å–ª–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü, –æ–±–Ω–æ–≤–ª—è–µ–º –æ—Ü–µ–Ω–∫—É
                if len(to_visit) > estimated_total_pages - pages_processed:
                    estimated_total_pages = pages_processed + len(to_visit)
                
                # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—É—â–µ–π –æ—Ü–µ–Ω–∫–∏ –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞
                if estimated_total_pages > 0:
                    percent = min(int((pages_processed / estimated_total_pages) * 100), 99)
                else:
                    percent = 0
                
                # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
                progress_bar_length = 20
                filled = int(progress_bar_length * percent / 100)
                bar = '‚ñà' * filled + '‚ñë' * (progress_bar_length - filled)
                
                # –°—Ç—Ä–æ–∏–º —Å–æ–æ–±—â–µ–Ω–∏–µ —Å –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º —Å—Ç–∞—Ç—É—Å–∞
                status_text = (
                    f"üîÑ <b>–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–∞–π—Ç–∞ {base_domain}...</b>\n\n"
                    f"üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: {percent}% [{bar}]\n"
                    f"üìë –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: <b>{pages_processed}</b>\n"
                    f"üìà –û—Ü–µ–Ω–∫–∞ –æ–±—â–µ–≥–æ –∫–æ–ª-–≤–∞ —Å—Ç—Ä–∞–Ω–∏—Ü: <b>~{estimated_total_pages}</b>\n"
                    f"üîó –¢–µ–∫—É—â–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞: <code>{current_url}</code>\n\n"
                    f"‚è≥ –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–¥–æ–∂–¥–∏—Ç–µ...\n"
                    f"<i>–ë–æ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü</i>"
                )
                
                try:
                    await status_message.edit_text(status_text, parse_mode=ParseMode.HTML, disable_web_page_preview=True)
                except Exception as e:
                    logger.warning(f"Couldn't update status message: {e}")

            logger.info(f"Processing ({pages_processed+1}/{estimated_total_pages}): {current_url}")
            visited.add(current_url)
            pages_processed += 1
            
            # –ó–∞–ø–æ–º–∏–Ω–∞–µ–º —Å–∫–æ–ª—å–∫–æ —Å—Å—ã–ª–æ–∫ –±—ã–ª–æ –¥–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            links_before = len(to_visit)

            html_content, error = await fetch_page(session, current_url)

            if error:
                errors.append(f"{current_url}: {error}")
                continue

            if html_content:
                # –ü–æ–ª—É—á–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                try:
                    page_soup = BeautifulSoup(html_content, 'html.parser')
                    page_title = page_soup.title.string if page_soup.title else "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"
                except:
                    page_title = "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ URL –ø–µ—Ä–µ–¥ —Ç–µ–∫—Å—Ç–æ–º
                page_separator = "\n\n" + "‚ïî" + "‚ïê" * 78 + "‚ïó\n"
                page_header = f"{page_separator}‚ïë  –°–¢–†–ê–ù–ò–¶–ê: {page_title.strip()}\n"
                page_header += f"‚ïë  URL: {current_url}\n"
                page_header += "‚ïö" + "‚ïê" * 78 + "‚ïù\n\n"
                
                # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ –¥–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                text = page_header + get_text_from_html(html_content)
                scraped_data[current_url] = text

                # Find links
                soup = BeautifulSoup(html_content, 'html.parser')
                for link in soup.find_all('a', href=True):
                    href = link['href']
                    absolute_url = urljoin(current_url, href)
                    parsed_absolute_url = urlparse(absolute_url)

                    # Check if internal link, http/https, and not visited/queued
                    if (parsed_absolute_url.netloc == base_domain and
                        parsed_absolute_url.scheme in ['http', 'https'] and
                        absolute_url not in visited and
                        absolute_url not in to_visit):
                        to_visit.append(absolute_url)
                
                # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º, —Å–∫–æ–ª—å–∫–æ –Ω–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫ –Ω–∞–π–¥–µ–Ω–æ –Ω–∞ —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ
                links_after = len(to_visit)
                new_links = links_after - links_before
                new_links_by_page[current_url] = new_links
                total_links_found += new_links
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ "–º–∞–ª–æ–ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ–π" –≤ –ø–ª–∞–Ω–µ –Ω–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫
                avg_links_per_page = total_links_found / max(1, pages_processed)
                if new_links < avg_links_per_page * 0.3 or new_links < discovery_threshold:
                    consecutive_low_discovery_pages += 1
                else:
                    consecutive_low_discovery_pages = 0
                
                # –ï—Å–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞–Ω–∏—Ü –ø–æ–¥—Ä—è–¥ –¥–∞—é—Ç –º–∞–ª–æ –Ω–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫ –∏ –º—ã –æ–±—Ä–∞–±–æ—Ç–∞–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü,
                # –º–æ–∂–Ω–æ –ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏—Ç—å, —á—Ç–æ –º—ã –∏—Å—Å–ª–µ–¥–æ–≤–∞–ª–∏ –±–æ–ª—å—à—É—é —á–∞—Å—Ç—å —Å–∞–π—Ç–∞
                if consecutive_low_discovery_pages >= max_consecutive_low_pages and pages_processed > 50:
                    logger.info(f"Stopping crawl after {pages_processed} pages due to low discovery rate")
                    break
                    
            # Small delay to be polite to the server
            await asyncio.sleep(0.1)

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å—Ç–∞—Ç—É—Å 100%
    if status_message:
        try:
            final_status = (
                f"‚úÖ <b>–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–∞–π—Ç–∞ {base_domain} –∑–∞–≤–µ—Ä—à–µ–Ω–∞!</b>\n\n"
                f"üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: 100% [{'‚ñà' * 20}]\n"
                f"üìë –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: <b>{pages_processed}</b>\n"
                f"‚è± –ó–∞–≤–µ—Ä—à–µ–Ω–æ!\n\n"
                f"üîÑ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤..."
            )
            await status_message.edit_text(final_status, parse_mode=ParseMode.HTML, disable_web_page_preview=True)
        except Exception as e:
            logger.warning(f"Couldn't update final status message: {e}")

    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å–∫—Ä–∞–ø–ª–µ–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –≤ –æ–¥–∏–Ω –¥–æ–∫—É–º–µ–Ω—Ç —Å –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ–º
    all_text = ""
    
    # –°–æ–∑–¥–∞–µ–º –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ
    toc = "\n\n" + "‚ïî" + "‚ïê" * 78 + "‚ïó\n"
    toc += "‚ïë" + " " * 30 + "–û–ì–õ–ê–í–õ–ï–ù–ò–ï" + " " * 30 + "‚ïë\n"
    toc += "‚ïö" + "‚ïê" * 78 + "‚ïù\n\n"
    
    page_number = 1
    titles = {}
    for url, text in scraped_data.items():
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞
        try:
            title_line = text.split("\n")[3]  # –°—Ç—Ä–æ–∫–∞ —Å "–°–¢–†–ê–ù–ò–¶–ê: ..."
            title = title_line.replace("–°–¢–†–ê–ù–ò–¶–ê: ", "").strip()
        except:
            title = f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_number}"
            
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è –æ–≥–ª–∞–≤–ª–µ–Ω–∏—è
        titles[url] = (title, page_number)
        page_number += 1
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –Ω–æ–º–µ—Ä—É —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    for url, (title, page_num) in sorted(titles.items(), key=lambda x: x[1][1]):
        toc += f"  {page_num:02d}. {title}\n"
    
    toc += "\n" + "‚ïî" + "‚ïê" * 78 + "‚ïó\n"
    toc += "‚ïë" + " " * 25 + "–ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –î–û–ö–£–ú–ï–ù–¢–ï" + " " * 25 + "‚ïë\n"
    toc += "‚ïö" + "‚ïê" * 78 + "‚ïù\n\n"
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–∞–π—Ç–µ
    domain_info = f"üìã –°–∞–π—Ç: {base_domain}\n"
    domain_info += f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {pages_processed}\n\n"
    
    # –°–æ–±–∏—Ä–∞–µ–º –∏—Ç–æ–≥–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
    all_text = domain_info + toc + "\n\n" + "‚ïî" + "‚ïê" * 78 + "‚ïó\n"
    all_text += "‚ïë" + " " * 27 + "–°–û–î–ï–†–ñ–ò–ú–û–ï –°–ê–ô–¢–ê" + " " * 27 + "‚ïë\n"
    all_text += "‚ïö" + "‚ïê" * 78 + "‚ïù\n\n"
    
    all_text += "\n".join(scraped_data.values())
    
    # –ï—Å–ª–∏ –±—ã–ª–∏ –æ—à–∏–±–∫–∏, –¥–æ–±–∞–≤–ª—è–µ–º –∏—Ö –≤ –∫–æ–Ω–µ—Ü
    if errors:
        error_section = "\n\n" + "‚ïî" + "‚ïê" * 78 + "‚ïó\n"
        error_section += "‚ïë" + " " * 26 + "–û–®–ò–ë–ö–ò –ü–†–ò –°–ö–ê–ù–ò–†–û–í–ê–ù–ò–ò" + " " * 25 + "‚ïë\n"
        error_section += "‚ïö" + "‚ïê" * 78 + "‚ïù\n\n"
        for error in errors:
            error_section += f"‚ö†Ô∏è {error}\n"
        all_text += error_section
    
    logger.info(f"Crawl completed: {pages_processed} pages processed")
    return all_text, pages_processed

def get_completion_reason(pages_processed, max_pages, consecutive_low_discovery):
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–∏—á–∏–Ω—É –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è."""
    if pages_processed >= max_pages:
        return "–î–æ—Å—Ç–∏–≥–Ω—É—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ª–∏–º–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü"
    elif consecutive_low_discovery >= 5:
        return "–ò—Å—á–µ—Ä–ø–∞–Ω—ã –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å–∞–π—Ç–∞"
    else:
        return "–ó–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ"

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Å–µ—Å—Å–∏—è–º–∏ –ò–ò
async def create_ai_session(user_id, username, first_name, document_text):
    """–°–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—É—é —Å–µ—Å—Å–∏—é —á–∞—Ç–∞ —Å –ò–ò."""
    db = get_db()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º –µ–≥–æ
    user = db.query(User).filter(User.telegram_id == user_id).first()
    if not user:
        user = User(telegram_id=user_id, username=username, first_name=first_name)
        db.add(user)
        db.commit()
        db.refresh(user)
    
    # –ó–∞–∫—Ä—ã–≤–∞–µ–º –≤—Å–µ –∞–∫—Ç–∏–≤–Ω—ã–µ —Å–µ—Å—Å–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    active_sessions = db.query(AISession).filter(
        AISession.user_id == user.id,
        AISession.is_active == True
    ).all()
    
    # –£–¥–∞–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –∏–∑ –∞–∫—Ç–∏–≤–Ω—ã—Ö —Å–µ—Å—Å–∏–π –∏ –∑–∞–∫—Ä—ã–≤–∞–µ–º –∏—Ö
    for session in active_sessions:
        # –£–¥–∞–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è
        db.query(DBMessage).filter(
            DBMessage.session_id == session.id
        ).delete()
        
        # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Å–µ—Å—Å–∏—é
        session.is_active = False
    
    # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —Å–µ—Å—Å–∏–∏ (–æ—Å—Ç–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5)
    old_sessions = db.query(AISession).filter(
        AISession.user_id == user.id,
        AISession.is_active == False
    ).order_by(AISession.last_activity.desc()).offset(5).all()
    
    for old_session in old_sessions:
        # –£–¥–∞–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è
        db.query(DBMessage).filter(
            DBMessage.session_id == old_session.id
        ).delete()
        
        # –£–¥–∞–ª—è–µ–º —Å–µ—Å—Å–∏—é
        db.delete(old_session)
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º datetime –±–µ–∑ UTC –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –ë–î
    current_time = datetime.datetime.now()
    
    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é —Å–µ—Å—Å–∏—é
    new_session = AISession(
        user_id=user.id,
        document_text=document_text,
        created_at=current_time,
        last_activity=current_time,
        is_active=True,
        request_count=0
    )
    
    db.add(new_session)
    db.commit()
    db.refresh(new_session)
    
    logger.info(f"Created new AI session {new_session.id} for user {user_id}")
    return new_session.id

async def get_active_session(user_id):
    """–ü–æ–ª—É—á–∞–µ—Ç –∞–∫—Ç–∏–≤–Ω—É—é —Å–µ—Å—Å–∏—é –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."""
    db = get_db()
    user = db.query(User).filter(User.telegram_id == user_id).first()
    
    if not user:
        return None
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∞–π–º–∞—É—Ç —Å–µ—Å—Å–∏–∏ - –∏—Å–ø–æ–ª—å–∑—É–µ–º datetime –±–µ–∑ UTC –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –ë–î
    timeout = datetime.datetime.now() - datetime.timedelta(minutes=SESSION_TIMEOUT_MINUTES)
    
    # –ù–∞—Ö–æ–¥–∏–º –∞–∫—Ç–∏–≤–Ω—É—é —Å–µ—Å—Å–∏—é
    session = db.query(AISession).filter(
        AISession.user_id == user.id,
        AISession.is_active == True
    ).first()
    
    # –ï—Å–ª–∏ —Å–µ—Å—Å–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞
    if not session:
        return None
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∞–π–º–∞—É—Ç –∏ –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤
    if session.last_activity <= timeout or session.request_count >= MAX_SESSION_REQUESTS:
        # –£–¥–∞–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è
        deleted_count = db.query(DBMessage).filter(
            DBMessage.session_id == session.id
        ).delete()
        
        # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Å–µ—Å—Å–∏—é
        session.is_active = False
        db.commit()
        
        reason = "timeout" if session.last_activity <= timeout else "request limit exceeded"
        logger.info(f"Session {session.id} closed due to {reason}. Deleted {deleted_count} messages.")
        return None
    
    return session

async def add_message_to_session(session_id, role, content):
    """–î–æ–±–∞–≤–ª—è–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ —Å–µ—Å—Å–∏—é."""
    db = get_db()
    session = db.query(AISession).filter(AISession.id == session_id).first()
    
    if not session:
        return False
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
    session.last_activity = datetime.datetime.now()
    
    # –ï—Å–ª–∏ —ç—Ç–æ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –∑–∞–ø—Ä–æ—Å–æ–≤
    if role == 'user':
        session.request_count += 1
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
    message = DBMessage(
        session_id=session_id,
        role=role,
        content=content,
        timestamp=datetime.datetime.now()
    )
    
    db.add(message)
    db.commit()
    
    return True

async def get_session_messages(session_id):
    """–ü–æ–ª—É—á–∞–µ—Ç –≤—Å–µ —Å–æ–æ–±—â–µ–Ω–∏—è –∏–∑ —Å–µ—Å—Å–∏–∏."""
    db = get_db()
    messages = db.query(DBMessage).filter(
        DBMessage.session_id == session_id
    ).order_by(DBMessage.timestamp).all()
    
    return [{'role': msg.role, 'content': msg.content} for msg in messages]

async def close_session(user_id):
    """–ó–∞–∫—Ä—ã–≤–∞–µ—Ç –∞–∫—Ç–∏–≤–Ω—É—é —Å–µ—Å—Å–∏—é –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ —É–¥–∞–ª—è–µ—Ç —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è."""
    db = get_db()
    user = db.query(User).filter(User.telegram_id == user_id).first()
    
    if not user:
        return False
    
    session = db.query(AISession).filter(
        AISession.user_id == user.id,
        AISession.is_active == True
    ).first()
    
    if session:
        # –£–¥–∞–ª—è–µ–º –≤—Å–µ —Å–æ–æ–±—â–µ–Ω–∏—è, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å —ç—Ç–æ–π —Å–µ—Å—Å–∏–µ–π
        deleted_messages = db.query(DBMessage).filter(
            DBMessage.session_id == session.id
        ).delete()
        
        # –ü–æ–º–µ—á–∞–µ–º —Å–µ—Å—Å–∏—é –∫–∞–∫ –Ω–µ–∞–∫—Ç–∏–≤–Ω—É—é
        session.is_active = False
        db.commit()
        
        logger.info(f"Session {session.id} for user {user_id} closed. Deleted {deleted_messages} messages.")
        return True
    
    return False

async def send_long_message(message, text, reply_markup=None):
    """
    –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –¥–ª–∏–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ, —Ä–∞–∑–±–∏–≤–∞—è –µ–≥–æ –Ω–∞ —á–∞—Å—Ç–∏ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏.
    """
    # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
    clean_text = clean_text_for_telegram(text)
    
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞—Å—Ç–∏, –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ
    parts = split_long_message(clean_text)
    
    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∫–∞–∂–¥—É—é —á–∞—Å—Ç—å
    for i, part in enumerate(parts):
        # –î–æ–±–∞–≤–ª—è–µ–º –º–∞—Ä–∫–µ—Ä "–ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ", –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ –ø–µ—Ä–≤–∞—è —á–∞—Å—Ç—å
        if i > 0:
            part = "... " + part
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–∞—Ä–∫–µ—Ä "–ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Å–ª–µ–¥—É–µ—Ç", –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ –ø–æ—Å–ª–µ–¥–Ω—è—è —á–∞—Å—Ç—å
        if i < len(parts) - 1:
            part += " ..."
        
        # –î–ª—è –ø–æ—Å–ª–µ–¥–Ω–µ–π —á–∞—Å—Ç–∏ –¥–æ–±–∞–≤–ª—è–µ–º reply_markup, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
        if i == len(parts) - 1 and reply_markup:
            await message.answer(part, reply_markup=reply_markup, disable_web_page_preview=True)
        else:
            await message.answer(part, disable_web_page_preview=True)

@dp.message(CommandStart())
async def send_welcome(message: Message, state: FSMContext):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /start."""
    user_id = message.from_user.id
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç—ã –¥–ª—è –∑–∞—â–∏—Ç—ã –æ—Ç DoS/DDoS
    is_allowed, reason, ban_time = rate_limiter.add_request(user_id, "general_requests")
    if not is_allowed:
        if reason == "banned":
            await message.answer(f"‚ö†Ô∏è –í—ã –≤—Ä–µ–º–µ–Ω–Ω–æ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω—ã –∏–∑-–∑–∞ –ø—Ä–µ–≤—ã—à–µ–Ω–∏—è –ª–∏–º–∏—Ç–∞ –∑–∞–ø—Ä–æ—Å–æ–≤. –ü–æ–≤—Ç–æ—Ä–∏—Ç–µ —á–µ—Ä–µ–∑ {ban_time} —Å–µ–∫—É–Ω–¥.")
        return
    
    logger.info(f"User {user_id} (@{message.from_user.username}) started the bot")
    
    await state.set_state(BotStates.NORMAL)
    
    welcome_message = (
        f"üëã <b>–î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å, {message.from_user.first_name}!</b>\n\n"
        f"üîç –Ø - –±–æ—Ç –¥–ª—è <b>—Å–∫—Ä–µ–π–ø–∏–Ω–≥–∞ –≤–µ–±-—Å–∞–π—Ç–æ–≤</b> —Å —Ñ—É–Ω–∫—Ü–∏—è–º–∏ –ò–ò.\n\n"
        f"<b>–ú–æ–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:</b>\n"
        f"üìö –°–±–æ—Ä —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å —Å–∞–π—Ç–æ–≤\n"
        f"ü§ñ –û—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ —Å–æ–±—Ä–∞–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º\n"
        f"üìù –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Å–ø–µ–∫—Ç–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n\n"
        f"<b>–ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:</b>\n"
        f"1Ô∏è‚É£ –û—Ç–ø—Ä–∞–≤—å—Ç–µ –º–Ω–µ URL —Å–∞–π—Ç–∞\n"
        f"2Ô∏è‚É£ –î–æ–∂–¥–∏—Ç–µ—Å—å –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö\n"
        f"3Ô∏è‚É£ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–Ω–æ–ø–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞\n\n"
        f"<i>–ù–∞—á–Ω–∏—Ç–µ —Å–µ–π—á–∞—Å - –ø—Ä–æ—Å—Ç–æ –æ—Ç–ø—Ä–∞–≤—å—Ç–µ URL!</i>"
    )
    
    await message.answer(welcome_message, parse_mode=ParseMode.HTML)

@dp.message(Command("stop"))
async def handle_stop_command(message: Message, state: FSMContext):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /stop - –∑–∞–∫—Ä—ã–≤–∞–µ—Ç —Å–µ—Å—Å–∏—é –ò–ò."""
    current_state = await state.get_state()
    
    if current_state == BotStates.AI_CHAT.state:
        await close_session(message.from_user.id)
        await state.set_state(BotStates.NORMAL)
        
        # –£–¥–∞–ª—è–µ–º –∫–ª–∞–≤–∏–∞—Ç—É—Ä—É
        await message.answer(
            "‚úÖ <b>–°–µ—Å—Å–∏—è —Å –ò–ò –∑–∞–≤–µ—Ä—à–µ–Ω–∞.</b>\n"
            "–¢–µ–ø–µ—Ä—å –≤—ã –º–æ–∂–µ—Ç–µ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –º–Ω–µ URL –¥–ª—è —Å–±–æ—Ä–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.",
            reply_markup=types.ReplyKeyboardRemove(),
            parse_mode=ParseMode.HTML
        )
    else:
        await message.answer(
            "‚ùì –£ –≤–∞—Å –Ω–µ—Ç –∞–∫—Ç–∏–≤–Ω–æ–π —Å–µ—Å—Å–∏–∏ —Å –ò–ò.\n"
            "–û—Ç–ø—Ä–∞–≤—å—Ç–µ URL —Å–∞–π—Ç–∞, —á—Ç–æ–±—ã –Ω–∞—á–∞—Ç—å —Ä–∞–±–æ—Ç—É."
        )

@dp.message(Command("admin"))
async def admin_command(message: Message):
    """–ö–æ–º–∞–Ω–¥–∞ –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–æ–º."""
    user_id = message.from_user.id
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–æ–º
    if user_id not in ADMIN_IDS:
        logger.warning(f"Unauthorized admin access attempt from user {user_id} (@{message.from_user.username})")
        return
    
    # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ—Ç rate_limiter
    stats = rate_limiter.get_stats()
    
    # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
    db = get_db()
    users_count = db.query(User).count()
    active_sessions = db.query(AISession).filter(AISession.is_active == True).count()
    total_sessions = db.query(AISession).count()
    total_messages = db.query(DBMessage).count()
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
    admin_message = (
        f"üìä <b>–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–æ—Ç–∞:</b>\n\n"
        f"üë§ <b>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏:</b>\n"
        f"- –í—Å–µ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –≤ –ë–î: <code>{users_count}</code>\n"
        f"- –ê–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: <code>{stats['total_users']}</code>\n"
        f"- –ó–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: <code>{stats['banned_users']}</code>\n\n"
        
        f"üí¨ <b>–°–µ—Å—Å–∏–∏ –∏ —Å–æ–æ–±—â–µ–Ω–∏—è:</b>\n"
        f"- –ê–∫—Ç–∏–≤–Ω—ã—Ö —Å–µ—Å—Å–∏–π –ò–ò: <code>{active_sessions}</code>\n"
        f"- –í—Å–µ–≥–æ —Å–µ—Å—Å–∏–π: <code>{total_sessions}</code>\n"
        f"- –í—Å–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏–π: <code>{total_messages}</code>\n\n"
        
        f"üîÑ <b>–ó–∞–ø—Ä–æ—Å—ã:</b>\n"
        f"- URL –∑–∞–ø—Ä–æ—Å—ã: <code>{stats['requests_by_type']['url_requests']}</code>\n"
        f"- –ò–ò –∑–∞–ø—Ä–æ—Å—ã: <code>{stats['requests_by_type']['ai_requests']}</code>\n"
        f"- –û–±—â–∏–µ –∑–∞–ø—Ä–æ—Å—ã: <code>{stats['requests_by_type']['general_requests']}</code>\n\n"
        
        f"‚è± <b>–î–∞–Ω–Ω—ã–µ —Å–æ–±—Ä–∞–Ω—ã:</b> <code>{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</code>"
    )
    
    await message.answer(admin_message, parse_mode=ParseMode.HTML)

@dp.message(lambda message: message.text == "üõë –ó–∞–≤–µ—Ä—à–∏—Ç—å —Å–µ—Å—Å–∏—é")
async def handle_end_session_button(message: Message, state: FSMContext):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –Ω–∞–∂–∞—Ç–∏—è –Ω–∞ –∫–Ω–æ–ø–∫—É –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Å–µ—Å—Å–∏–∏."""
    current_state = await state.get_state()
    
    if current_state == BotStates.AI_CHAT.state:
        await close_session(message.from_user.id)
        await state.set_state(BotStates.NORMAL)
        
        # –£–¥–∞–ª—è–µ–º –∫–ª–∞–≤–∏–∞—Ç—É—Ä—É
        await message.answer(
            "‚úÖ <b>–°–µ—Å—Å–∏—è —Å –ò–ò –∑–∞–≤–µ—Ä—à–µ–Ω–∞.</b>\n"
            "–¢–µ–ø–µ—Ä—å –≤—ã –º–æ–∂–µ—Ç–µ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –º–Ω–µ URL –¥–ª—è —Å–±–æ—Ä–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.",
            reply_markup=types.ReplyKeyboardRemove(),
            parse_mode=ParseMode.HTML
        )

@dp.callback_query(lambda c: c.data == 'ask_ai')
async def process_ask_ai_button(callback_query: types.CallbackQuery, state: FSMContext):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –Ω–∞–∂–∞—Ç–∏—è –Ω–∞ –∏–Ω–ª–∞–π–Ω –∫–Ω–æ–ø–∫—É '–°–ø—Ä–æ—Å–∏—Ç—å –ò–ò'."""
    user_id = callback_query.from_user.id
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç—ã –¥–ª—è –∑–∞—â–∏—Ç—ã –æ—Ç DoS/DDoS
    is_allowed, reason, ban_time = rate_limiter.add_request(user_id, "general_requests")
    if not is_allowed:
        if reason == "banned":
            await callback_query.message.answer(f"‚ö†Ô∏è –í—ã –≤—Ä–µ–º–µ–Ω–Ω–æ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω—ã –∏–∑-–∑–∞ –ø—Ä–µ–≤—ã—à–µ–Ω–∏—è –ª–∏–º–∏—Ç–∞ –∑–∞–ø—Ä–æ—Å–æ–≤. –ü–æ–≤—Ç–æ—Ä–∏—Ç–µ —á–µ—Ä–µ–∑ {ban_time} —Å–µ–∫—É–Ω–¥.")
        await callback_query.answer()
        return
    
    # –°—Ä–∞–∑—É –æ—Ç–≤–µ—á–∞–µ–º –Ω–∞ callback query, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å —Ç–∞–π–º–∞—É—Ç–∞
    await callback_query.answer()
    
    # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ —Å–æ—Å—Ç–æ—è–Ω–∏—è
    data = await state.get_data()
    document_text = data.get("document_text")
    
    if not document_text:
        await callback_query.message.answer("‚ùå –°–Ω–∞—á–∞–ª–∞ –æ—Ç–ø—Ä–∞–≤—å—Ç–µ URL –¥–ª—è —Å–±–æ—Ä–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.")
        return
    
    # –°–æ–∑–¥–∞–µ–º —Å–µ—Å—Å–∏—é –≤ –ë–î
    session_id = await create_ai_session(
        callback_query.from_user.id,
        callback_query.from_user.username,
        callback_query.from_user.first_name,
        document_text
    )
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º ID —Å–µ—Å—Å–∏–∏ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏
    await state.update_data(ai_session_id=session_id)
    
    # –ü–µ—Ä–µ–≤–æ–¥–∏–º –±–æ—Ç–∞ –≤ —Ä–µ–∂–∏–º AI_CHAT
    await state.set_state(BotStates.AI_CHAT)
    
    # –°–æ–∑–¥–∞–µ–º –∫–ª–∞–≤–∏–∞—Ç—É—Ä—É —Å –∫–Ω–æ–ø–∫–æ–π –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Å–µ—Å—Å–∏–∏
    keyboard = ReplyKeyboardMarkup(
        keyboard=[[KeyboardButton(text="üõë –ó–∞–≤–µ—Ä—à–∏—Ç—å —Å–µ—Å—Å–∏—é")]],
        resize_keyboard=True
    )
    
    ai_chat_message = (
        f"ü§ñ <b>–†–µ–∂–∏–º —á–∞—Ç–∞ —Å –ò–ò –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω!</b>\n\n"
        f"üìù –ó–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É –¥–æ–∫—É–º–µ–Ω—Ç–∞.\n\n"
        f"‚ÑπÔ∏è <b>–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–µ—Å—Å–∏–∏:</b>\n"
        f"üìä –î–æ—Å—Ç—É–ø–Ω–æ –∑–∞–ø—Ä–æ—Å–æ–≤: <b>{MAX_SESSION_REQUESTS}</b>\n"
        f"‚è± –í—Ä–µ–º—è –±–µ–∑–¥–µ–π—Å—Ç–≤–∏—è –¥–æ –∑–∞–∫—Ä—ã—Ç–∏—è: <b>{SESSION_TIMEOUT_MINUTES} –º–∏–Ω</b>\n\n"
        f"<i>–î–ª—è –≤—ã—Ö–æ–¥–∞ –Ω–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É –Ω–∏–∂–µ.</i>"
    )
    
    await callback_query.message.answer(
        ai_chat_message,
        reply_markup=keyboard,
        parse_mode=ParseMode.HTML
    )

@dp.callback_query(lambda c: c.data == 'get_summary')
async def process_summary_button(callback_query: types.CallbackQuery, state: FSMContext):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –Ω–∞–∂–∞—Ç–∏—è –Ω–∞ –∏–Ω–ª–∞–π–Ω –∫–Ω–æ–ø–∫—É '–ö–æ–Ω—Å–ø–µ–∫—Ç'."""
    user_id = callback_query.from_user.id
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç—ã –¥–ª—è –∑–∞—â–∏—Ç—ã –æ—Ç DoS/DDoS
    is_allowed, reason, ban_time = rate_limiter.add_request(user_id, "ai_requests")
    if not is_allowed:
        if reason == "banned":
            await callback_query.message.answer(f"‚ö†Ô∏è –í—ã –≤—Ä–µ–º–µ–Ω–Ω–æ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω—ã –∏–∑-–∑–∞ –ø—Ä–µ–≤—ã—à–µ–Ω–∏—è –ª–∏–º–∏—Ç–∞ –∑–∞–ø—Ä–æ—Å–æ–≤. –ü–æ–≤—Ç–æ—Ä–∏—Ç–µ —á–µ—Ä–µ–∑ {ban_time} —Å–µ–∫—É–Ω–¥.")
        await callback_query.answer()
        return
    
    # –°—Ä–∞–∑—É –æ—Ç–≤–µ—á–∞–µ–º –Ω–∞ callback query, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å —Ç–∞–π–º–∞—É—Ç–∞
    await callback_query.answer()
    
    # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞, ID –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ —Ç–∏–ø –æ–±—Ö–æ–¥–∞ –∏–∑ —Å–æ—Å—Ç–æ—è–Ω–∏—è
    data = await state.get_data()
    document_text = data.get("document_text")
    cached_document_id = data.get("cached_document_id")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ –æ–±—Ö–æ–¥–æ–º –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    is_single_page = data.get("is_single_page", False)
    
    if not document_text:
        await callback_query.message.answer("–°–Ω–∞—á–∞–ª–∞ –æ—Ç–ø—Ä–∞–≤—å—Ç–µ URL –¥–ª—è —Å–±–æ—Ä–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.")
        return
    
    # –°–æ–∑–¥–∞–µ–º –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ —Å –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–º –∑–∞–≥—Ä—É–∑–∫–∏
    loading_message = await callback_query.message.answer(
        "ü§ñ –ì–µ–Ω–µ—Ä–∏—Ä—É—é –∫–æ–Ω—Å–ø–µ–∫—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞...\n\n"
        "‚è≥ –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –æ–∫–æ–ª–æ –º–∏–Ω—É—Ç—ã. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–¥–æ–∂–¥–∏—Ç–µ..."
    )
    
    # –ï—Å–ª–∏ –µ—Å—Ç—å ID –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞, –ø—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à –∫–æ–Ω—Å–ø–µ–∫—Ç–æ–≤
    if cached_document_id:
        cached_summary = get_cached_summary(cached_document_id, is_single_page=is_single_page)
        if cached_summary:
            logger.info(f"Using cached summary for document {cached_document_id} (single_page: {is_single_page})")
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –æ –∑–∞–≥—Ä—É–∑–∫–µ
            await loading_message.edit_text(
                "‚úÖ –ö–æ–Ω—Å–ø–µ–∫—Ç –≥–æ—Ç–æ–≤! –û—Ç–ø—Ä–∞–≤–ª—è—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã...\n"
                "<i>–î–∞–Ω–Ω—ã–µ –ø–æ–ª—É—á–µ–Ω—ã –∏–∑ –∫—ç—à–∞</i>", 
                parse_mode=ParseMode.HTML,
                disable_web_page_preview=True
            )
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
            await send_long_message(callback_query.message, cached_summary)
            
            # –£–¥–∞–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –æ –∑–∞–≥—Ä—É–∑–∫–µ
            await callback_query.bot.delete_message(chat_id=callback_query.message.chat.id, message_id=loading_message.message_id)
            
            return
    
    # –ï—Å–ª–∏ –∫–æ–Ω—Å–ø–µ–∫—Ç–∞ –Ω–µ—Ç –≤ –∫—ç—à–µ –∏–ª–∏ –Ω–µ—Ç ID –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–æ–≤—ã–π
    
    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –Ω–∞–±–æ—Ä–∞ —Ç–µ–∫—Å—Ç–∞
    await callback_query.bot.send_chat_action(chat_id=callback_query.message.chat.id, action="typing")
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å —Ç–µ–∫—Å—Ç–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –∫–æ–Ω—Å–ø–µ–∫—Ç–∞
    system_prompt = AI_SUMMARY_PROMPT.format(document_text=document_text)
    
    # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç –ò–ò (–∫–æ–Ω—Å–ø–µ–∫—Ç)
    try:
        ai_response = await ai_client.get_completion([{"role": "user", "content": "–°–æ—Å—Ç–∞–≤—å –∫–æ–Ω—Å–ø–µ–∫—Ç"}], system_prompt)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –æ—Ç–≤–µ—Ç –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ
        if ai_response and not ai_response.startswith("–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –æ—Ç–≤–µ—Ç–∞"):
            # –ï—Å–ª–∏ –µ—Å—Ç—å ID –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Å–ø–µ–∫—Ç –≤ –∫—ç—à
            if cached_document_id:
                cache_summary(cached_document_id, ai_response, is_single_page=is_single_page)
                logger.info(f"Cached summary for document {cached_document_id} (single_page: {is_single_page})")
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –æ –∑–∞–≥—Ä—É–∑–∫–µ
            await loading_message.edit_text("‚úÖ –ö–æ–Ω—Å–ø–µ–∫—Ç –≥–æ—Ç–æ–≤! –û—Ç–ø—Ä–∞–≤–ª—è—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã...", disable_web_page_preview=True)
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
            await send_long_message(callback_query.message, ai_response)
            
            # –£–¥–∞–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –æ –∑–∞–≥—Ä—É–∑–∫–µ
            await callback_query.bot.delete_message(chat_id=callback_query.message.chat.id, message_id=loading_message.message_id)
        else:
            # –ï—Å–ª–∏ –≤ –æ—Ç–≤–µ—Ç–µ —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è –æ—à–∏–±–∫–∞, –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –µ—ë
            error_message = ai_response if ai_response else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∫–æ–Ω—Å–ø–µ–∫—Ç–∞"
            await loading_message.edit_text(f"‚ùå {error_message}", disable_web_page_preview=True)
            logger.error(f"AI response error for user {user_id}: {ai_response}")
    except Exception as e:
        logger.exception(f"Error generating summary for user {user_id}: {e}")
        await loading_message.edit_text(f"‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∫–æ–Ω—Å–ø–µ–∫—Ç–∞: {e}", disable_web_page_preview=True)

@dp.message(BotStates.AI_CHAT)
async def handle_ai_chat_message(message: Message, state: FSMContext):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ —Ä–µ–∂–∏–º–µ —á–∞—Ç–∞ —Å –ò–ò."""
    user_id = message.from_user.id
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç—ã –¥–ª—è –∑–∞—â–∏—Ç—ã –æ—Ç DoS/DDoS
    is_allowed, reason, ban_time = rate_limiter.add_request(user_id, "ai_requests")
    if not is_allowed:
        if reason == "banned":
            await message.answer(f"‚ö†Ô∏è –í—ã –≤—Ä–µ–º–µ–Ω–Ω–æ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω—ã –∏–∑-–∑–∞ –ø—Ä–µ–≤—ã—à–µ–Ω–∏—è –ª–∏–º–∏—Ç–∞ –∑–∞–ø—Ä–æ—Å–æ–≤. –ü–æ–≤—Ç–æ—Ä–∏—Ç–µ —á–µ—Ä–µ–∑ {ban_time} —Å–µ–∫—É–Ω–¥.")
        return
    
    # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —Å–æ—Å—Ç–æ—è–Ω–∏—è
    data = await state.get_data()
    session_id = data.get("ai_session_id")
    
    if not session_id:
        await message.answer("–û—à–∏–±–∫–∞: —Å–µ—Å—Å–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –Ω–∞—á–Ω–∏—Ç–µ –∑–∞–Ω–æ–≤–æ.")
        await state.set_state(BotStates.NORMAL)
        return
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å–µ—Å—Å–∏—è –∞–∫—Ç–∏–≤–Ω–∞
    session = await get_active_session(user_id)
    if not session:
        await message.answer(
            "–í–∞—à–∞ —Å–µ—Å—Å–∏—è —Å –ò–ò –±—ã–ª–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∏–∑-–∑–∞ —Ç–∞–π–º–∞—É—Ç–∞ –∏–ª–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–∏—è –ª–∏–º–∏—Ç–∞ –∑–∞–ø—Ä–æ—Å–æ–≤. "
            "–ù–∞—á–Ω–∏—Ç–µ –Ω–æ–≤—É—é —Å–µ—Å—Å–∏—é, –æ—Ç–ø—Ä–∞–≤–∏–≤ URL.",
            reply_markup=types.ReplyKeyboardRemove()
        )
        await state.set_state(BotStates.NORMAL)
        return
    
    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –æ —Ç–æ–º, —á—Ç–æ –∑–∞–ø—Ä–æ—Å –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è
    loading_message = await message.answer(
        f"ü§î –û–±–¥—É–º—ã–≤–∞—é –≤–∞—à –≤–æ–ø—Ä–æ—Å... ({session.request_count + 1}/{MAX_SESSION_REQUESTS})"
    )
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –ë–î
    await add_message_to_session(session_id, 'user', message.text)
    
    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Å–æ–æ–±—â–µ–Ω–∏—è –∏–∑ —Å–µ—Å—Å–∏–∏
    messages = await get_session_messages(session_id)
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å —Ç–µ–∫—Å—Ç–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞
    system_prompt = AI_SYSTEM_PROMPT_TEMPLATE.format(document_text=session.document_text)
    
    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –Ω–∞–±–æ—Ä–∞ —Ç–µ–∫—Å—Ç–∞
    await message.bot.send_chat_action(chat_id=message.chat.id, action="typing")
    
    # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç –ò–ò
    ai_response = await ai_client.get_completion(messages, system_prompt)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç –ò–ò –≤ –ë–î
    await add_message_to_session(session_id, 'assistant', ai_response)
    
    # –£–¥–∞–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –æ –∑–∞–≥—Ä—É–∑–∫–µ
    await message.bot.delete_message(chat_id=message.chat.id, message_id=loading_message.message_id)
    
    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –¥–ª–∏–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
    await send_long_message(message, ai_response)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –∏—Å—á–µ—Ä–ø–∞–Ω –ª–∏ –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤
    if session.request_count >= MAX_SESSION_REQUESTS:
        await message.answer(
            f"‚ö†Ô∏è –í—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –≤—Å–µ {MAX_SESSION_REQUESTS} –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —ç—Ç–æ–π —Å–µ—Å—Å–∏–∏. –°–µ—Å—Å–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞.",
            reply_markup=types.ReplyKeyboardRemove()
        )
        await close_session(user_id)
        await state.set_state(BotStates.NORMAL)

@dp.message()
async def handle_message(message: Message, state: FSMContext):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Ö–æ–¥—è—â–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è, –æ–∂–∏–¥–∞—è URL."""
    user_id = message.from_user.id
    
    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ–º–∞–Ω–¥—ã
    if message.text and message.text.startswith('/'):
        return
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
    current_state = await state.get_state()
    if current_state == BotStates.AI_CHAT.state:
        # –î–µ–ª–µ–≥–∏—Ä—É–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —á–∞—Ç–∞ —Å –ò–ò
        await handle_ai_chat_message(message, state)
        return
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç—ã –¥–ª—è –∑–∞—â–∏—Ç—ã –æ—Ç DoS/DDoS
    is_allowed, reason, ban_time = rate_limiter.add_request(user_id, "general_requests")
    if not is_allowed:
        if reason == "banned":
            await message.answer(f"‚ö†Ô∏è –í—ã –≤—Ä–µ–º–µ–Ω–Ω–æ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω—ã –∏–∑-–∑–∞ –ø—Ä–µ–≤—ã—à–µ–Ω–∏—è –ª–∏–º–∏—Ç–∞ –∑–∞–ø—Ä–æ—Å–æ–≤. –ü–æ–≤—Ç–æ—Ä–∏—Ç–µ —á–µ—Ä–µ–∑ {ban_time} —Å–µ–∫—É–Ω–¥.")
        return
    
    url = message.text
    if not url or not is_valid_url(url):
        await message.answer("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ—Ç–ø—Ä–∞–≤—å—Ç–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π URL (–Ω–∞–ø—Ä–∏–º–µ—Ä, https://example.com).")
        return

    # Add scheme if missing for initial validation and filename generation
    if not url.startswith(('http://', 'https://')):
        url = 'https://' + url

    parsed_initial_url = urlparse(url)
    if not all([parsed_initial_url.scheme, parsed_initial_url.netloc]):
        await message.answer("–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π URL. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –æ–Ω —Å–æ–¥–µ—Ä–∂–∏—Ç http:// –∏–ª–∏ https:// –∏ –¥–æ–º–µ–Ω–Ω–æ–µ –∏–º—è.")
        return

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç—ã –¥–ª—è URL –∑–∞–ø—Ä–æ—Å–æ–≤
    is_allowed, reason, ban_time = rate_limiter.add_request(user_id, "url_requests")
    if not is_allowed:
        if reason == "banned":
            await message.answer(f"‚ö†Ô∏è –í—ã –≤—Ä–µ–º–µ–Ω–Ω–æ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω—ã –∏–∑-–∑–∞ –ø—Ä–µ–≤—ã—à–µ–Ω–∏—è –ª–∏–º–∏—Ç–∞ –∑–∞–ø—Ä–æ—Å–æ–≤. –ü–æ–≤—Ç–æ—Ä–∏—Ç–µ —á–µ—Ä–µ–∑ {ban_time} —Å–µ–∫—É–Ω–¥.")
        elif reason.startswith("rate_limit_exceeded"):
            await message.answer(f"‚ö†Ô∏è –ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–¥–æ–∂–¥–∏—Ç–µ –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–æ–π.")
        return

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º URL –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏
    await state.update_data(url=url)
    
    # –°–æ–∑–¥–∞–µ–º –∏–Ω–ª–∞–π–Ω –∫–Ω–æ–ø–∫–∏ –¥–ª—è –≤—ã–±–æ—Ä–∞ —Ä–µ–∂–∏–º–∞ —Å–∫—Ä–µ–π–ø–∏–Ω–≥–∞
    inline_kb = InlineKeyboardMarkup(inline_keyboard=[
        [InlineKeyboardButton(text="üìÑ –¢–æ–ª—å–∫–æ —Ç–µ–∫—É—â–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞", callback_data="crawl_single")],
        [InlineKeyboardButton(text="üåê –ü–æ–ª–Ω—ã–π –æ–±—Ö–æ–¥ —Å–∞–π—Ç–∞", callback_data="crawl_full")]
    ])
    
    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ —Å –≤—ã–±–æ—Ä–æ–º —Ä–µ–∂–∏–º–∞
    choice_message = (
        f"üîç <b>–í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª—è —Å–∞–π—Ç–∞:</b> <code>{parsed_initial_url.netloc}</code>\n\n"
        f"<b>üìÑ –¢–æ–ª—å–∫–æ —Ç–µ–∫—É—â–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞</b>\n"
        f"- –ë—ã—Å—Ç—Ä–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Ç–æ–ª—å–∫–æ —Å —É–∫–∞–∑–∞–Ω–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã\n"
        f"- –ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –∏–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n\n"
        f"<b>üåê –ü–æ–ª–Ω—ã–π –æ–±—Ö–æ–¥ —Å–∞–π—Ç–∞</b>\n"
        f"- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏ –æ–±—Ö–æ–¥ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü\n"
        f"- –ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤—Å–µ–≥–æ —Å–∞–π—Ç–∞ (–∑–∞–π–º–µ—Ç –±–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏)\n\n"
        f"üëá <b>–°–¥–µ–ª–∞–π—Ç–µ –≤—ã–±–æ—Ä:</b>"
    )
    
    await message.answer(choice_message, reply_markup=inline_kb, parse_mode=ParseMode.HTML, disable_web_page_preview=True)


@dp.callback_query(lambda c: c.data == 'crawl_single')
async def process_single_page_crawl(callback_query: types.CallbackQuery, state: FSMContext):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –≤—ã–±–æ—Ä–∞ –æ–±—Ö–æ–¥–∞ —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã."""
    user_id = callback_query.from_user.id
    
    # –°—Ä–∞–∑—É –æ—Ç–≤–µ—á–∞–µ–º –Ω–∞ callback query, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å —Ç–∞–π–º–∞—É—Ç–∞
    await callback_query.answer()
    
    # –ü–æ–ª—É—á–∞–µ–º URL –∏–∑ —Å–æ—Å—Ç–æ—è–Ω–∏—è
    data = await state.get_data()
    url = data.get("url")
    
    if not url:
        await callback_query.message.answer("–û—à–∏–±–∫–∞: URL –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ—Ç–ø—Ä–∞–≤—å—Ç–µ URL –∑–∞–Ω–æ–≤–æ.")
        return

    parsed_url = urlparse(url)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à –¥–ª—è —ç—Ç–æ–≥–æ URL (—Å —Ñ–ª–∞–≥–æ–º is_single_page=True)
    cached_doc = get_cached_document(url, is_single_page=True)
    
    # –ï—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞–π–¥–µ–Ω –≤ –∫—ç—à–µ
    if cached_doc:
        logger.info(f"Using cached document for URL {url} (single page)")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ —Å —Ñ–ª–∞–≥–æ–º –æ–¥–∏–Ω–æ—á–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        await state.update_data(
            document_text=cached_doc["content"], 
            cached_document_id=cached_doc["id"],
            is_single_page=True
        )
        
        # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª –≤ –ø–∞–º—è—Ç–∏
        file_content = io.BytesIO(cached_doc["content"].encode('utf-8', errors='ignore'))
        filename_base = parsed_url.netloc or "scraped_page"
        filename = "".join(c if c.isalnum() or c in ('-', '_', '.') else '_' for c in filename_base) + "_single_cached.txt"
        
        # –°–æ–∑–¥–∞–µ–º –∏–Ω–ª–∞–π–Ω –∫–Ω–æ–ø–∫–∏ –¥–ª—è –¥–µ–π—Å—Ç–≤–∏–π —Å –¥–æ–∫—É–º–µ–Ω—Ç–æ–º
        inline_kb = InlineKeyboardMarkup(inline_keyboard=[
            [InlineKeyboardButton(text="ü§ñ –°–ø—Ä–æ—Å–∏—Ç—å –ò–ò –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ", callback_data="ask_ai")],
            [InlineKeyboardButton(text="üìù –ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Å–ø–µ–∫—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞", callback_data="get_summary")]
        ])
        
        input_file = BufferedInputFile(file_content.getvalue(), filename=filename)
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –ø–æ–º–µ—Ç–∫–æ–π –æ –∫—ç—à–µ
        result_caption = (
            f"üìÑ <b>–¢–µ–∫—Å—Ç —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã:</b> <code>{url}</code>\n\n"
            f"üìä <b>–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:</b>\n"
            f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: <b>1</b>\n"
            f"üì¶ –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: <b>{len(cached_doc['content']) // 1024} –ö–±</b>\n"
            f"üîÑ <i>–î–∞–Ω–Ω—ã–µ –ø–æ–ª—É—á–µ–Ω—ã –∏–∑ –∫—ç—à–∞</i>\n\n"
            f"üëá <b>–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ:</b>"
        )
        
        await callback_query.message.answer_document(
            input_file,
            caption=result_caption,
            reply_markup=inline_kb,
            parse_mode=ParseMode.HTML,
            disable_web_page_preview=True
        )
        
        logger.info(f"Sent cached single page content for {url} to user {user_id}")
        return

    # –ï—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –∫—ç—à–µ, –≤—ã–ø–æ–ª–Ω—è–µ–º –æ–±—Ö–æ–¥
    # –°–æ–∑–¥–∞–µ–º —Å—Ç–∞—Ä—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
    start_message = (
        f"üåê <b>–ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É —Å—Ç—Ä–∞–Ω–∏—Ü—ã:</b> <code>{parsed_url.netloc}</code>\n\n"
        f"üîç –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ —Å–∫—Ä–µ–π–ø–∏–Ω–≥—É...\n"
        f"‚öôÔ∏è –†–µ–∂–∏–º: <b>–û–¥–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞</b>\n\n"
        f"‚è≥ –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–¥–æ–∂–¥–∏—Ç–µ..."
    )
    status_message = await callback_query.message.answer(start_message, parse_mode=ParseMode.HTML, disable_web_page_preview=True)
    
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é crawl_website —Å max_pages=1 –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        scraped_text, pages_count = await crawl_website(url, max_pages=1, status_message=status_message)

        if scraped_text.startswith("Error:"):
            await status_message.edit_text(scraped_text, disable_web_page_preview=True)
            return
        
        if not scraped_text.strip() or pages_count == 0:
            await status_message.edit_text("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ —É–∫–∞–∑–∞–Ω–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ.", disable_web_page_preview=True)
            return
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –≤ –∫—ç—à
        document_id = cache_document(url, scraped_text, pages_count, is_single_page=True)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ ID –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ —Å —Ñ–ª–∞–≥–æ–º –æ–¥–∏–Ω–æ—á–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        await state.update_data(
            document_text=scraped_text, 
            cached_document_id=document_id,
            is_single_page=True
        )
        
        # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª –≤ –ø–∞–º—è—Ç–∏
        file_content = io.BytesIO(scraped_text.encode('utf-8', errors='ignore'))
        filename_base = parsed_url.netloc or "scraped_page"
        filename = "".join(c if c.isalnum() or c in ('-', '_', '.') else '_' for c in filename_base) + "_single.txt"
        
        # –°–æ–∑–¥–∞–µ–º –∏–Ω–ª–∞–π–Ω –∫–Ω–æ–ø–∫–∏ –¥–ª—è –¥–µ–π—Å—Ç–≤–∏–π —Å –¥–æ–∫—É–º–µ–Ω—Ç–æ–º
        inline_kb = InlineKeyboardMarkup(inline_keyboard=[
            [InlineKeyboardButton(text="ü§ñ –°–ø—Ä–æ—Å–∏—Ç—å –ò–ò –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ", callback_data="ask_ai")],
            [InlineKeyboardButton(text="üìù –ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Å–ø–µ–∫—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞", callback_data="get_summary")]
        ])
        
        input_file = BufferedInputFile(file_content.getvalue(), filename=filename)
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result_caption = (
            f"üìÑ <b>–¢–µ–∫—Å—Ç —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã:</b> <code>{url}</code>\n\n"
            f"üìä <b>–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏:</b>\n"
            f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: <b>1</b>\n"
            f"üì¶ –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: <b>{len(scraped_text) // 1024} –ö–±</b>\n\n"
            f"üëá <b>–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ:</b>"
        )
        
        await callback_query.message.answer_document(
            input_file,
            caption=result_caption,
            reply_markup=inline_kb,
            parse_mode=ParseMode.HTML,
            disable_web_page_preview=True
        )
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ç—É—Å–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
        await status_message.delete()
        
        logger.info(f"Sent single page content for {url} to user {user_id}")
        
    except Exception as e:
        logger.exception(f"Error processing single page crawl for URL {url} from user {user_id}: {e}")
        await status_message.edit_text(f"‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –Ω–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞: {e}", disable_web_page_preview=True)


@dp.callback_query(lambda c: c.data == 'crawl_full')
async def process_full_crawl(callback_query: types.CallbackQuery, state: FSMContext):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –≤—ã–±–æ—Ä–∞ –ø–æ–ª–Ω–æ–≥–æ –æ–±—Ö–æ–¥–∞ —Å–∞–π—Ç–∞."""
    user_id = callback_query.from_user.id
    
    # –°—Ä–∞–∑—É –æ—Ç–≤–µ—á–∞–µ–º –Ω–∞ callback query, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å —Ç–∞–π–º–∞—É—Ç–∞
    await callback_query.answer()
    
    # –ü–æ–ª—É—á–∞–µ–º URL –∏–∑ —Å–æ—Å—Ç–æ—è–Ω–∏—è
    data = await state.get_data()
    url = data.get("url")
    
    if not url:
        await callback_query.message.answer("–û—à–∏–±–∫–∞: URL –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ—Ç–ø—Ä–∞–≤—å—Ç–µ URL –∑–∞–Ω–æ–≤–æ.")
        return

    parsed_initial_url = urlparse(url)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à –¥–ª—è —ç—Ç–æ–≥–æ URL (—Å —Ñ–ª–∞–≥–æ–º is_single_page=False)
    cached_doc = get_cached_document(url, is_single_page=False)
    
    # –ï—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞–π–¥–µ–Ω –≤ –∫—ç—à–µ
    if cached_doc:
        logger.info(f"Using cached document for URL {url} (full crawl)")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ —Å —Ñ–ª–∞–≥–æ–º –ø–æ–ª–Ω–æ–≥–æ –æ–±—Ö–æ–¥–∞
        await state.update_data(
            document_text=cached_doc["content"], 
            cached_document_id=cached_doc["id"],
            is_single_page=False
        )
        
        # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª –≤ –ø–∞–º—è—Ç–∏
        file_content = io.BytesIO(cached_doc["content"].encode('utf-8', errors='ignore'))
        filename_base = parsed_initial_url.netloc or "scraped_site"
        filename = "".join(c if c.isalnum() or c in ('-', '_', '.') else '_' for c in filename_base) + "_full_cached.txt"
        
        # –°–æ–∑–¥–∞–µ–º –∏–Ω–ª–∞–π–Ω –∫–Ω–æ–ø–∫–∏ –¥–ª—è –¥–µ–π—Å—Ç–≤–∏–π —Å –¥–æ–∫—É–º–µ–Ω—Ç–æ–º
        inline_kb = InlineKeyboardMarkup(inline_keyboard=[
            [InlineKeyboardButton(text="ü§ñ –°–ø—Ä–æ—Å–∏—Ç—å –ò–ò –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ", callback_data="ask_ai")],
            [InlineKeyboardButton(text="üìù –ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Å–ø–µ–∫—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞", callback_data="get_summary")]
        ])
        
        input_file = BufferedInputFile(file_content.getvalue(), filename=filename)
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –ø–æ–º–µ—Ç–∫–æ–π –æ –∫—ç—à–µ
        result_caption = (
            f"üìÑ <b>–¢–µ–∫—Å—Ç —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü —Å–∞–π—Ç–∞:</b> <code>{parsed_initial_url.netloc}</code>\n\n"
            f"üìä <b>–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:</b>\n"
            f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: <b>{cached_doc['pages_processed']}</b>\n"
            f"üì¶ –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: <b>{len(cached_doc['content']) // 1024} –ö–±</b>\n"
            f"üîÑ <i>–î–∞–Ω–Ω—ã–µ –ø–æ–ª—É—á–µ–Ω—ã –∏–∑ –∫—ç—à–∞</i>\n\n"
            f"üëá <b>–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ:</b>"
        )
        
        await callback_query.message.answer_document(
            input_file,
            caption=result_caption,
            reply_markup=inline_kb,
            parse_mode=ParseMode.HTML,
            disable_web_page_preview=True
        )
        
        logger.info(f"Sent cached full crawl content for {url} to user {user_id}")
        return
    
    # –ï—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –∫—ç—à–µ, –≤—ã–ø–æ–ª–Ω—è–µ–º –æ–±—Ö–æ–¥
    # –°–æ–∑–¥–∞–µ–º —Å—Ç–∞—Ä—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
    start_message = (
        f"üåê <b>–ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É —Å–∞–π—Ç–∞:</b> <code>{parsed_initial_url.netloc}</code>\n\n"
        f"üîç –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ —Å–∫—Ä–µ–π–ø–∏–Ω–≥—É...\n"
        f"‚öôÔ∏è –ú–∞–∫—Å–∏–º—É–º —Å—Ç—Ä–∞–Ω–∏—Ü: <b>1000</b>\n"
        f"üï∏Ô∏è –¢–∏–ø –æ–±—Ö–æ–¥–∞: <b>–ü–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º —Å—Å—ã–ª–∫–∞–º</b>\n\n"
        f"‚è≥ –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–¥–æ–∂–¥–∏—Ç–µ..."
    )
    status_message = await callback_query.message.answer(start_message, parse_mode=ParseMode.HTML, disable_web_page_preview=True)

    try:
        scraped_text, pages_count = await crawl_website(url, max_pages=1000, status_message=status_message)

        if scraped_text.startswith("Error:"):
             await status_message.edit_text(scraped_text, disable_web_page_preview=True)
             return

        if not scraped_text.strip() or pages_count == 0:
            await status_message.edit_text("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –∏–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –æ–±—Ö–æ–¥–∞.", disable_web_page_preview=True)
            return

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –≤ –∫—ç—à
        document_id = cache_document(url, scraped_text, pages_count, is_single_page=False)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ ID –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ —Å —Ñ–ª–∞–≥–æ–º –ø–æ–ª–Ω–æ–≥–æ –æ–±—Ö–æ–¥–∞
        await state.update_data(
            document_text=scraped_text, 
            cached_document_id=document_id,
            is_single_page=False
        )

        # Create a file in memory
        file_content = io.BytesIO(scraped_text.encode('utf-8', errors='ignore'))
        # Sanitize filename
        filename_base = parsed_initial_url.netloc or "scraped_site"
        filename = "".join(c if c.isalnum() or c in ('-', '_', '.') else '_' for c in filename_base) + "_full.txt"

        # –°–æ–∑–¥–∞–µ–º –∏–Ω–ª–∞–π–Ω –∫–Ω–æ–ø–∫–∏ "–°–ø—Ä–æ—Å–∏—Ç—å –ò–ò" –∏ "–ö–æ–Ω—Å–ø–µ–∫—Ç"
        inline_kb = InlineKeyboardMarkup(inline_keyboard=[
            [InlineKeyboardButton(text="ü§ñ –°–ø—Ä–æ—Å–∏—Ç—å –ò–ò –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ", callback_data="ask_ai")],
            [InlineKeyboardButton(text="üìù –ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Å–ø–µ–∫—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞", callback_data="get_summary")]
        ])

        input_file = BufferedInputFile(file_content.getvalue(), filename=filename)

        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        result_caption = (
            f"üìÑ <b>–¢–µ–∫—Å—Ç —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü —Å–∞–π—Ç–∞:</b> <code>{parsed_initial_url.netloc}</code>\n\n"
            f"üìä <b>–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏:</b>\n"
            f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: <b>{pages_count}</b>\n"
            f"üì¶ –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: <b>{len(scraped_text) // 1024} –ö–±</b>\n\n"
            f"üëá <b>–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ:</b>"
        )

        await callback_query.message.answer_document(
            input_file, 
            caption=result_caption,
            reply_markup=inline_kb,
            parse_mode=ParseMode.HTML,
            disable_web_page_preview=True
        )
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ç—É—Å–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
        await status_message.delete()
        
        logger.info(f"Sent crawled content for {url} ({pages_count} pages) to user {user_id}")

    except Exception as e:
        logger.exception(f"Error handling full crawl for URL {url} from user {user_id}: {e}")
        await status_message.edit_text(f"‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –Ω–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞: {e}", disable_web_page_preview=True)

@dp.message(Command("cleanup_cache"))
async def clear_cache_command(message: Message):
    """
    –ö–æ–º–∞–Ω–¥–∞ –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ —Å—Ç–∞—Ä–æ–≥–æ –∫—ç—à–∞ (–¥–æ—Å—Ç—É–ø–Ω–∞ —Ç–æ–ª—å–∫–æ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–∞–º).
    –£–¥–∞–ª—è–µ—Ç –∑–∞–ø–∏—Å–∏ –∫—ç—à–∞, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –±–æ–ª–µ–µ 30 –¥–Ω–µ–π.
    """
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–æ–º
    if message.from_user.id not in ADMIN_IDS:
        await message.answer("‚ö†Ô∏è –£ –≤–∞—Å –Ω–µ—Ç –ø—Ä–∞–≤ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —ç—Ç–æ–π –∫–æ–º–∞–Ω–¥—ã.")
        return
    
    try:
        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏ –∫—ç—à–∞ (—Å—Ç–∞—Ä—à–µ 30 –¥–Ω–µ–π)
        removed_count = cleanup_old_cache(days_threshold=30)
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –æ—á–∏—Å—Ç–∫–∏
        await message.answer(
            f"‚úÖ –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!\n\n"
            f"üóë –£–¥–∞–ª–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: <b>{removed_count}</b>", 
            parse_mode=ParseMode.HTML
        )
        logger.info(f"Cache cleanup completed by admin {message.from_user.id}, removed {removed_count} entries")
    except Exception as e:
        await message.answer(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ –∫—ç—à–∞: {e}")
        logger.exception(f"Error during cache cleanup: {e}")


@dp.message(Command("cache_stats"))
async def cache_stats_command(message: Message):
    """
    –ö–æ–º–∞–Ω–¥–∞ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫—ç—à–∞ (–¥–æ—Å—Ç—É–ø–Ω–∞ —Ç–æ–ª—å–∫–æ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–∞–º).
    """
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–æ–º
    if message.from_user.id not in ADMIN_IDS:
        await message.answer("‚ö†Ô∏è –£ –≤–∞—Å –Ω–µ—Ç –ø—Ä–∞–≤ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —ç—Ç–æ–π –∫–æ–º–∞–Ω–¥—ã.")
        return
    
    try:
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫—ç—à–∞
        stats = cache_stats()
        
        # –ï—Å–ª–∏ –≤ –∫—ç—à–µ –µ—Å—Ç—å –∑–∞–ø–∏—Å–∏, –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ–¥—Ä–æ–±–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        if stats["docs_count"] > 0:
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            stats_text = (
                f"üìä <b>–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—ç—à–∞:</b>\n\n"
                f"üìë <b>–î–æ–∫—É–º–µ–Ω—Ç—ã:</b>\n"
                f"- –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: <b>{stats['docs_count']}</b>\n"
                f"  ‚Ä¢ –û–¥–∏–Ω–æ—á–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü: <b>{stats['single_page_docs']}</b>\n"
                f"  ‚Ä¢ –ü–æ–ª–Ω—ã—Ö –æ–±—Ö–æ–¥–æ–≤: <b>{stats['full_crawl_docs']}</b>\n"
                f"- –í—Å–µ–≥–æ –∫–æ–Ω—Å–ø–µ–∫—Ç–æ–≤: <b>{stats['summaries_count']}</b>\n"
                f"  ‚Ä¢ –î–ª—è –æ–¥–∏–Ω–æ—á–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü: <b>{stats['single_page_summaries']}</b>\n"
                f"  ‚Ä¢ –î–ª—è –ø–æ–ª–Ω—ã—Ö –æ–±—Ö–æ–¥–æ–≤: <b>{stats['full_crawl_summaries']}</b>\n"
                f"- –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞: <b>{stats['avg_doc_size_kb']:.1f} –ö–±</b>\n\n"
            )
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–∞–º—ã—Ö –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
            if stats["most_popular_doc"]:
                stats_text += (
                    f"üîç <b>–°–∞–º—ã–π –ø–æ–ø—É–ª—è—Ä–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç:</b>\n"
                    f"- URL: <code>{stats['most_popular_doc'].url[:50]}...</code>\n"
                    f"- –ü—Ä–æ—Å–º–æ—Ç—Ä–æ–≤: <b>{stats['most_popular_doc'].access_count}</b>\n"
                    f"- –¢–∏–ø: <b>{'–û–¥–∏–Ω–æ—á–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞' if stats['most_popular_doc'].is_single_page else '–ü–æ–ª–Ω—ã–π –æ–±—Ö–æ–¥'}</b>\n"
                    f"- –†–∞–∑–º–µ—Ä: <b>{len(stats['most_popular_doc'].content) // 1024} –ö–±</b>\n\n"
                )
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–ø—É–ª—è—Ä–Ω–æ–π –æ–¥–∏–Ω–æ—á–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ, –µ—Å–ª–∏ –µ—Å—Ç—å
            if stats["most_popular_page"]:
                stats_text += (
                    f"üìÑ <b>–ü–æ–ø—É–ª—è—Ä–Ω–∞—è –æ–¥–∏–Ω–æ—á–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞:</b>\n"
                    f"- URL: <code>{stats['most_popular_page'].url[:50]}...</code>\n"
                    f"- –ü—Ä–æ—Å–º–æ—Ç—Ä–æ–≤: <b>{stats['most_popular_page'].access_count}</b>\n\n"
                )
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–ø—É–ª—è—Ä–Ω–æ–º –ø–æ–ª–Ω–æ–º –æ–±—Ö–æ–¥–µ, –µ—Å–ª–∏ –µ—Å—Ç—å
            if stats["most_popular_full"]:
                stats_text += (
                    f"üåê <b>–ü–æ–ø—É–ª—è—Ä–Ω—ã–π –ø–æ–ª–Ω—ã–π –æ–±—Ö–æ–¥:</b>\n"
                    f"- URL: <code>{stats['most_popular_full'].url[:50]}...</code>\n"
                    f"- –ü—Ä–æ—Å–º–æ—Ç—Ä–æ–≤: <b>{stats['most_popular_full'].access_count}</b>\n"
                    f"- –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: <b>{stats['most_popular_full'].pages_processed}</b>\n\n"
                )
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤—Ä–µ–º–µ–Ω–∏ —Å–æ–∑–¥–∞–Ω–∏—è
            if stats["oldest_doc_date"] and stats["newest_doc_date"]:
                stats_text += (
                    f"‚è±Ô∏è <b>–í—Ä–µ–º–µ–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:</b>\n"
                    f"- –°–∞–º—ã–π —Å—Ç–∞—Ä—ã–π –¥–æ–∫—É–º–µ–Ω—Ç: <b>{stats['oldest_doc_date'].strftime('%d.%m.%Y %H:%M')}</b>\n"
                    f"- –°–∞–º—ã–π –Ω–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç: <b>{stats['newest_doc_date'].strftime('%d.%m.%Y %H:%M')}</b>\n\n"
                )
            
            stats_text += f"üïí <i>–î–∞–Ω–Ω—ã–µ —Å–æ–±—Ä–∞–Ω—ã: {datetime.datetime.now().strftime('%d.%m.%Y %H:%M:%S')}</i>"
        else:
            stats_text = "üìä <b>–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—ç—à–∞:</b>\n\n–ö—ç—à –ø—É—Å—Ç."
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        await message.answer(stats_text, parse_mode=ParseMode.HTML, disable_web_page_preview=True)
        logger.info(f"Cache stats requested by admin {message.from_user.id}")
    except Exception as e:
        await message.answer(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫—ç—à–∞: {e}")
        logger.exception(f"Error getting cache stats: {e}")

async def main() -> None:
    """Initializes and starts the bot."""
    # Initialize database
    init_db()

    # Initialize Bot with default session settings.
    # No custom session or SSL context passed.
    bot = Bot(
        token=BOT_TOKEN,
        default=DefaultBotProperties(parse_mode=ParseMode.HTML)
    )

    # Run bot polling using default session management.
    await dp.start_polling(bot)


if __name__ == '__main__':
    # Ensure the main function handles potential exceptions during session creation/closing
    try:
        asyncio.run(main())
    except (KeyboardInterrupt, SystemExit):
        logger.info("Bot stopped!")
    except Exception as e:
        logger.critical(f"Critical error during bot execution: {e}", exc_info=True)
